import torch
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, GenerationConfig
from peft import get_peft_model, LoraConfig, TaskType

LOCAL_MODEL_PATH = "../ai_models/gemma-3-270m"
DTYPE = torch.bfloat16
MODEL_OPTION = {"use_safetensors": True}
ADAPTER_FLAG = False
ADAPTER_PATH = ""

device = None
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("âœ… Using Apple Silicon GPU (MPS)")
else:
    device = torch.device("cpu")
    print("âš ï¸ MPS not available; using CPU")

# ==== í† í¬ë‚˜ì´ì € ë¡œë“œ ====
def get_tokenizer(model_path):
    print("ğŸ”„ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=True,
        padding_side="left",  # ë°°ì¹˜ ì¶”ë¡  ëŒ€ë¹„ ì•ˆì „
        use_safetensors=True,
    )
    if tokenizer.pad_token is None:
        print("âš ï¸ pad_tokenì´ ì—†ì–´ì„œ eos_tokenìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    tokenizer.padding_side = "left"
    return tokenizer

def get_model(model_path, dtype, option):
    print("ğŸ”„ Loading model...")
    return AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=dtype,
        low_cpu_mem_usage=True,
        use_safetensors=option["use_safetensors"],
    )

def set_model_to_device(model, device):
    print("ğŸ”„ Moving model to device...")
    model.to(device)
    model.eval()
    return model

tokenizer = get_tokenizer(LOCAL_MODEL_PATH)
model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)
model = set_model_to_device(model, device)

print("load done.")

# ==== í”„ë¡¬í”„íŠ¸ êµ¬ì„±: chat í…œí”Œë¦¿ ìë™ ê°ì§€ ====
def build_inputs(user_text: str):
    # tokenizerê°€ chat í…œí”Œë¦¿ì„ ì œê³µí•˜ë©´ ê·¸ê±¸ ì‚¬ìš© (instruct ëª¨ë¸ì— ìœ ë¦¬)
    has_chat = hasattr(tokenizer, "chat_template") and tokenizer.chat_template
    if has_chat:
        messages = [{"role": "user", "content": user_text}]
        enc = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        )
    else:
        # ë² ì´ìŠ¤ ëª¨ë¸ì¼ ê²½ìš° ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸
        enc = tokenizer(
            user_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        )
    return enc

# ì˜µì…˜ì— ë”°ë¥¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
# í¼í¬ë¨¼ìŠ¤ í–¥ìƒ í…ŒìŠ¤íŠ¸
print("===== generate =====")
# gen_cfg = GenerationConfig(
#     max_new_tokens=256,       # ë„ˆë¬´ ê¸¸ë©´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ 128~256 ê¶Œì¥
#     temperature=None,      # ë‹¤ì–‘ì„± í™•ë³´
#     top_p=None,
#     do_sample=False,           # ë‹¤ì–‘ì„± í™•ë³´
#     repetition_penalty=1.05,  # ì¤‘ë³µ ì–µì œ ì‚´ì§
#     eos_token_id=tokenizer.eos_token_id,
#     pad_token_id=tokenizer.eos_token_id,  # pad ê²½ê³  ë°©ì§€
# )

system_prompt = """ë„ˆëŠ” ì¸ê³µì§€ëŠ¥ ëŒ€í™” ëª¨ë¸ì´ì•¼.
ëŒ€ë‹µì€ ê°„ë‹¨í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ í•´ì•¼ í•´.
ìì‹ ì— ëŒ€í•´ ë¬¼ìœ¼ë©´ "ì €ëŠ” Gemma 3 270M ëª¨ë¸ì…ë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µí•´."""
user_input = "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."

format_user_input = f"{system_prompt}\nì‚¬ìš©ì: {user_input}\në‹µë³€:"
inputs = tokenizer(format_user_input, return_tensors="pt")
device = model.device
inputs = {k: v.to(device) for k, v in inputs.items()}
with torch.no_grad():
    gen_cfg = GenerationConfig(
        max_new_tokens=256,
        do_sample=False,           # ìƒ˜í”Œë§ ë”
        repetition_penalty=1.2,
        no_repeat_ngram_size=3,
        num_beams=3,               # ë¹”ì„œì¹˜
        early_stopping=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

    # gen_cfg.to_dict()ë¥¼ json pretty printë¡œ í™•ì¸í•´ë³´ì„¸ìš”.
    json_pretty = gen_cfg.to_dict()
    import json
    print(json.dumps(json_pretty, indent=2, ensure_ascii=False))

    outputs = model.generate(
        **inputs,
        **gen_cfg.to_dict()
    )

# ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œì™¸)
gen_only = outputs[0][inputs["input_ids"].shape[-1]:]
text = tokenizer.decode(gen_only, skip_special_tokens=True)
raw = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\n===== MODEL RAW OUTPUT =====")
print(raw)
print("========================")

print("\n===== MODEL OUTPUT =====")
print(text)
print("========================")