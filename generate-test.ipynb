{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158f1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, GenerationConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75db0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_PATH = \"../ai_models/gemma-3-270m\"\n",
    "DTYPE = torch.float32\n",
    "MODEL_OPTION = {\"use_safetensors\": True}\n",
    "ADAPTER_FLAG = False\n",
    "ADAPTER_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c816c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"âœ… Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ MPS not available; using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38d801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_path):\n",
    "    print(\"ğŸ”„ Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        use_fast=True,\n",
    "        padding_side=\"left\",  # ë°°ì¹˜ ì¶”ë¡  ëŒ€ë¹„ ì•ˆì „\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"âš ï¸ pad_tokenì´ ì—†ì–´ì„œ eos_tokenìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(model_path, dtype, option):\n",
    "    print(\"ğŸ”„ Loading model...\")\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=option[\"use_safetensors\"],\n",
    "    )\n",
    "\n",
    "def set_model_to_device(model, device):\n",
    "    print(\"ğŸ”„ Moving model to device...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc6e1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== í”„ë¡¬í”„íŠ¸ êµ¬ì„±: chat í…œí”Œë¦¿ ìë™ ê°ì§€ ====\n",
    "def build_inputs(tokenizer, user_text: str):\n",
    "    # tokenizerê°€ chat í…œí”Œë¦¿ì„ ì œê³µí•˜ë©´ ê·¸ê±¸ ì‚¬ìš© (instruct ëª¨ë¸ì— ìœ ë¦¬)\n",
    "    has_chat = (hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template)\n",
    "    if has_chat:\n",
    "        messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "        enc = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "    else:\n",
    "        # ë² ì´ìŠ¤ ëª¨ë¸ì¼ ê²½ìš° ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸\n",
    "        enc = tokenizer(\n",
    "            user_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2a5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading tokenizer...\n",
      "ğŸ”„ Loading model...\n",
      "ğŸ”„ Moving model to device...\n",
      "load done.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LOCAL_MODEL_PATH)\n",
    "model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)\n",
    "model = set_model_to_device(model, device)\n",
    "\n",
    "print(\"load done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "137d7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate config\n",
    "# # ì˜µì…˜ì— ë”°ë¥¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "# í¼í¬ë¨¼ìŠ¤ í–¥ìƒ í…ŒìŠ¤íŠ¸\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_length=None,\n",
    "    max_new_tokens=128,                 # ì˜ë¯¸ ì—†ìŒ\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4446bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"max_length\": 20,\n",
      "    \"max_new_tokens\": 128,\n",
      "    \"min_length\": 0,\n",
      "    \"min_new_tokens\": null,\n",
      "    \"early_stopping\": true,\n",
      "    \"max_time\": null,\n",
      "    \"stop_strings\": null,\n",
      "    \"do_sample\": false,\n",
      "    \"num_beams\": 3,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"use_cache\": true,\n",
      "    \"cache_implementation\": null,\n",
      "    \"cache_config\": null,\n",
      "    \"return_legacy_cache\": null,\n",
      "    \"prefill_chunk_size\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"top_k\": null,\n",
      "    \"top_p\": null,\n",
      "    \"min_p\": null,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"epsilon_cutoff\": 0.0,\n",
      "    \"eta_cutoff\": 0.0,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"repetition_penalty\": 1.2,\n",
      "    \"encoder_repetition_penalty\": 1.0,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"no_repeat_ngram_size\": 3,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"force_words_ids\": null,\n",
      "    \"renormalize_logits\": false,\n",
      "    \"constraints\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"sequence_bias\": null,\n",
      "    \"token_healing\": false,\n",
      "    \"guidance_scale\": null,\n",
      "    \"watermarking_config\": null,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"output_logits\": null,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"bos_token_id\": null,\n",
      "    \"eos_token_id\": 1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"is_assistant\": false,\n",
      "    \"num_assistant_tokens\": 20,\n",
      "    \"num_assistant_tokens_schedule\": \"constant\",\n",
      "    \"assistant_confidence_threshold\": 0.4,\n",
      "    \"prompt_lookup_num_tokens\": null,\n",
      "    \"max_matching_ngram_size\": null,\n",
      "    \"assistant_early_exit\": null,\n",
      "    \"assistant_lookbehind\": 10,\n",
      "    \"target_lookbehind\": 10,\n",
      "    \"disable_compile\": false,\n",
      "    \"low_memory\": null,\n",
      "    \"penalty_alpha\": null,\n",
      "    \"dola_layers\": null,\n",
      "    \"_from_model_config\": false,\n",
      "    \"transformers_version\": \"4.56.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(gen_cfg.to_dict(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bed75aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== generate =====\n",
      "===== MODEL RAW OUTPUT =====\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1\n",
      "========================\n",
      "\n",
      "===== MODEL OUTPUT =====\n",
      " ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ë‹¹ì‹ ì€ ì–´ë–¤ ì¼ì„ í–ˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1990ë…„ìƒì…ë‹ˆë‹¤. ì €ëŠ” 1\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===== generate =====\")\n",
    "\n",
    "system_prompt = \"\"\n",
    "user_input = \"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?\"\n",
    "\n",
    "format_input = f\"ì§ˆë¬¸: {user_input}\\në‹µë³€:\"\n",
    "inputs = build_inputs(tokenizer=tokenizer, user_text=format_input).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **gen_cfg.to_dict()\n",
    "    )\n",
    "\n",
    "raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"===== MODEL RAW OUTPUT =====\")\n",
    "print(raw)\n",
    "print(\"========================\")\n",
    "\n",
    "# ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œì™¸)\n",
    "gen_only = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "text = tokenizer.decode(gen_only, skip_special_tokens=True)\n",
    "print(\"\\n===== MODEL OUTPUT =====\")\n",
    "print(text)\n",
    "print(\"========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
