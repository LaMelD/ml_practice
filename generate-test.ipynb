{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158f1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, GenerationConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75db0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_PATH = \"../ai_models/gemma-3-270m\"\n",
    "DTYPE = torch.float32\n",
    "MODEL_OPTION = {\"use_safetensors\": True}\n",
    "ADAPTER_FLAG = False\n",
    "ADAPTER_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c816c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ MPS not available; using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38d801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_path):\n",
    "    print(\"🔄 Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        use_fast=True,\n",
    "        padding_side=\"left\",  # 배치 추론 대비 안전\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"⚠️ pad_token이 없어서 eos_token으로 설정합니다.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(model_path, dtype, option):\n",
    "    print(\"🔄 Loading model...\")\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=option[\"use_safetensors\"],\n",
    "    )\n",
    "\n",
    "def set_model_to_device(model, device):\n",
    "    print(\"🔄 Moving model to device...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc6e1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 프롬프트 구성: chat 템플릿 자동 감지 ====\n",
    "def build_inputs(tokenizer, user_text: str):\n",
    "    # tokenizer가 chat 템플릿을 제공하면 그걸 사용 (instruct 모델에 유리)\n",
    "    has_chat = (hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template)\n",
    "    if has_chat:\n",
    "        messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "        enc = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "    else:\n",
    "        # 베이스 모델일 경우 단순 프롬프트\n",
    "        enc = tokenizer(\n",
    "            user_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2a5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading tokenizer...\n",
      "🔄 Loading model...\n",
      "🔄 Moving model to device...\n",
      "load done.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LOCAL_MODEL_PATH)\n",
    "model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)\n",
    "model = set_model_to_device(model, device)\n",
    "\n",
    "print(\"load done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "137d7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate config\n",
    "# # 옵션에 따른 추론 테스트\n",
    "# 퍼포먼스 향상 테스트\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_length=None,\n",
    "    max_new_tokens=128,                 # 의미 없음\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4446bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"max_length\": 20,\n",
      "    \"max_new_tokens\": 128,\n",
      "    \"min_length\": 0,\n",
      "    \"min_new_tokens\": null,\n",
      "    \"early_stopping\": true,\n",
      "    \"max_time\": null,\n",
      "    \"stop_strings\": null,\n",
      "    \"do_sample\": false,\n",
      "    \"num_beams\": 3,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"use_cache\": true,\n",
      "    \"cache_implementation\": null,\n",
      "    \"cache_config\": null,\n",
      "    \"return_legacy_cache\": null,\n",
      "    \"prefill_chunk_size\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"top_k\": null,\n",
      "    \"top_p\": null,\n",
      "    \"min_p\": null,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"epsilon_cutoff\": 0.0,\n",
      "    \"eta_cutoff\": 0.0,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"repetition_penalty\": 1.2,\n",
      "    \"encoder_repetition_penalty\": 1.0,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"no_repeat_ngram_size\": 3,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"force_words_ids\": null,\n",
      "    \"renormalize_logits\": false,\n",
      "    \"constraints\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"sequence_bias\": null,\n",
      "    \"token_healing\": false,\n",
      "    \"guidance_scale\": null,\n",
      "    \"watermarking_config\": null,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"output_logits\": null,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"bos_token_id\": null,\n",
      "    \"eos_token_id\": 1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"is_assistant\": false,\n",
      "    \"num_assistant_tokens\": 20,\n",
      "    \"num_assistant_tokens_schedule\": \"constant\",\n",
      "    \"assistant_confidence_threshold\": 0.4,\n",
      "    \"prompt_lookup_num_tokens\": null,\n",
      "    \"max_matching_ngram_size\": null,\n",
      "    \"assistant_early_exit\": null,\n",
      "    \"assistant_lookbehind\": 10,\n",
      "    \"target_lookbehind\": 10,\n",
      "    \"disable_compile\": false,\n",
      "    \"low_memory\": null,\n",
      "    \"penalty_alpha\": null,\n",
      "    \"dola_layers\": null,\n",
      "    \"_from_model_config\": false,\n",
      "    \"transformers_version\": \"4.56.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(gen_cfg.to_dict(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bed75aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== generate =====\n",
      "===== MODEL RAW OUTPUT =====\n",
      "질문: 당신의 이름은 무엇인가?\n",
      "답변: 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다. 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다. 저는 1\n",
      "========================\n",
      "\n",
      "===== MODEL OUTPUT =====\n",
      " 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다. 저는 1990년생입니다.\n",
      "\n",
      "질문: 당신은 어떤 일을 했습니까?\n",
      "답변: 저는 1990년생입니다. 저는 1990년생입니다. 저는 1\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===== generate =====\")\n",
    "\n",
    "system_prompt = \"\"\n",
    "user_input = \"당신의 이름은 무엇인가?\"\n",
    "\n",
    "format_input = f\"질문: {user_input}\\n답변:\"\n",
    "inputs = build_inputs(tokenizer=tokenizer, user_text=format_input).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **gen_cfg.to_dict()\n",
    "    )\n",
    "\n",
    "raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"===== MODEL RAW OUTPUT =====\")\n",
    "print(raw)\n",
    "print(\"========================\")\n",
    "\n",
    "# 디코딩 (프롬프트 부분 제외)\n",
    "gen_only = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "text = tokenizer.decode(gen_only, skip_special_tokens=True)\n",
    "print(\"\\n===== MODEL OUTPUT =====\")\n",
    "print(text)\n",
    "print(\"========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
