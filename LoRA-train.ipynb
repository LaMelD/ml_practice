{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a15f2e",
   "metadata": {},
   "source": [
    "# Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec936bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv torch transformers datasets bitsandbytes accelerate peft -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3fc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# ==== MPS ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï ====\n",
    "def get_device():\n",
    "    device = None\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS ÎîîÎ∞îÏù¥Ïä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPSÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏñ¥ CPUÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    return device\n",
    "\n",
    "# ==== ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ====\n",
    "def get_tokenizer(model_path):\n",
    "    print(\"üîÑ Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        use_fast=True,\n",
    "        padding_side=\"left\",  # Î∞∞Ïπò Ï∂îÎ°† ÎåÄÎπÑ ÏïàÏ†Ñ\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"‚ö†Ô∏è pad_tokenÏù¥ ÏóÜÏñ¥ÏÑú eos_tokenÏúºÎ°ú ÏÑ§Ï†ïÌï©ÎãàÎã§.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.padding_side = \"left\"  \n",
    "    return tokenizer\n",
    "\n",
    "def get_model(model_path, dtype, option):\n",
    "    print(\"üîÑ Loading model...\")\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=option[\"use_safetensors\"],\n",
    "    )\n",
    "\n",
    "def set_model_to_device(model, device):\n",
    "    print(\"üîÑ Moving model to device...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1eb80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS ÎîîÎ∞îÏù¥Ïä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
      "üîÑ Loading tokenizer...\n",
      "üîÑ Loading model...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangjoon/.pyenv/versions/ai_practice/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 339,341,312 || all params: 607,439,488 || trainable%: 55.8642\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "LOCAL_MODEL_PATH = \"../ai_models/gemma-3-270m\"\n",
    "DTYPE = torch.bfloat16\n",
    "MODEL_OPTION = {\"use_safetensors\": True}\n",
    "ADAPTER_FLAG = False\n",
    "ADAPTER_PATH = \"\"\n",
    "\n",
    "device = get_device()\n",
    "tokenizer = get_tokenizer(LOCAL_MODEL_PATH)\n",
    "model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    # target_modules = [\"c_attn\", \"c_proj\", \"q_attn\"], # GPT Í≥ÑÏó¥\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    modules_to_save=['embed_tokens', 'lm_head'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.to(\"mps\")\n",
    "model.eval()\n",
    "\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a01d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# jsonl ÌååÏùºÏùÑ Î∂àÎü¨ÏôÄ dataset ÏÉùÏÑ±\n",
    "dataset = Dataset.from_json(\"./ecommerce_data/ecommerce_finetune.jsonl\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e388de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 2796.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_train(train_data):\n",
    "    return f\"ÏßàÎ¨∏: {train_data['input']}\\nÎãµÎ≥Ä: {train_data['output']}\\n<END>\"\n",
    "\n",
    "def tokenize_func(train_data):\n",
    "    return tokenizer(\n",
    "        format_train(train_data),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,   # Î™®Îç∏ context ÌÅ¨Í∏∞Ïóê ÎßûÍ≤å Ï°∞Ï†ï\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "eos_token_id=tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=15,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1919fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangjoon/.pyenv/versions/ai_practice/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:19, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    mask = labels != -100\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ba44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏßàÎ¨∏:ÏßÄÎÇúÎã¨ Ïã†Í∑ú Í∞ÄÏûÖ Í≥†Í∞ù ÏàòÎäî?(SQLÎßå ÏûëÏÑ±)\n",
      "ÎãµÎ≥Ä: SELECT COUNT(*) FROM users WHERE signup_date >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') AND signup_date < DATE_TRUNC('month', CURRENT_DATE);\n",
      "Î¨∏Ïùò ÏûêÍ≤©: SELECT COUNT(*) FROM users GROUP BY signup_date;\n",
      "Ìï¥\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ÏßàÎ¨∏:ÏßÄÎÇúÎã¨ Ïã†Í∑ú Í∞ÄÏûÖ Í≥†Í∞ù ÏàòÎäî?(SQLÎßå ÏûëÏÑ±)\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "device = model.device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    eos_token_id=eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fd6e6",
   "metadata": {},
   "source": [
    "#### RAG\n",
    "\n",
    "1. ÌååÏùº Î°úÎìú\n",
    "\n",
    "2. ÏûÑÎ≤†Îî© :: vector DB Î•º ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò / Ï†ÄÏû•\n",
    "\n",
    "3. vector DB Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ :: chroma ÏÇ¨Ïö© -> in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b034da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ÌååÏùº Î°úÎìú\n",
    "rag_dataset = load_dataset(\"json\", data_files=\"./ecommerce_data/ecommerce_schema_rag.jsonl\")\n",
    "\n",
    "print(rag_dataset)\n",
    "\n",
    "\n",
    "# ÏûÑÎ≤†Îî© :: vector DB Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌï¥ Îç∞Ïù¥ÌÑ∞Î•º Î≥ÄÌôò / Ï†ÄÏû•\n",
    "# vector DB Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ :: chroma ÏÇ¨Ïö© -> in memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
