{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a15f2e",
   "metadata": {},
   "source": [
    "# Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec936bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv torch transformers datasets bitsandbytes accelerate peft -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3fc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# ==== MPS ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï ====\n",
    "def get_device():\n",
    "    device = None\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS ÎîîÎ∞îÏù¥Ïä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPSÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏñ¥ CPUÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    return device\n",
    "\n",
    "# ==== ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ====\n",
    "def get_tokenizer(model_path):\n",
    "    print(\"üîÑ Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        use_fast=True,\n",
    "        padding_side=\"left\",  # Î∞∞Ïπò Ï∂îÎ°† ÎåÄÎπÑ ÏïàÏ†Ñ\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"‚ö†Ô∏è pad_tokenÏù¥ ÏóÜÏñ¥ÏÑú eos_tokenÏúºÎ°ú ÏÑ§Ï†ïÌï©ÎãàÎã§.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.padding_side = \"left\"  \n",
    "    return tokenizer\n",
    "\n",
    "def get_model(model_path, dtype, option):\n",
    "    print(\"üîÑ Loading model...\")\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=option[\"use_safetensors\"],\n",
    "    )\n",
    "\n",
    "def set_model_to_device(model, device):\n",
    "    print(\"üîÑ Moving model to device...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1eb80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS ÎîîÎ∞îÏù¥Ïä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
      "üîÑ Loading tokenizer...\n",
      "üîÑ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/Users/sangjoon/.pyenv/versions/ai_practice/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 339,343,872 || all params: 607,443,328 || trainable%: 55.8643\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "LOCAL_MODEL_PATH = \"../ai_models/gemma-3-270m\"\n",
    "DTYPE = torch.bfloat16\n",
    "MODEL_OPTION = {\"use_safetensors\": True}\n",
    "ADAPTER_FLAG = False\n",
    "ADAPTER_PATH = \"\"\n",
    "\n",
    "device = get_device()\n",
    "tokenizer = get_tokenizer(LOCAL_MODEL_PATH)\n",
    "model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    # target_modules = [\"c_attn\", \"c_proj\", \"q_attn\"], # GPT Í≥ÑÏó¥\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    modules_to_save=['embed_tokens', 'lm_head'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['<END>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.to(\"mps\")\n",
    "model.eval()\n",
    "\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"validation\". Should be one of ['train', 'test'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[32m      5\u001b[39m train_dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mgretelai/synthetic_text_to_sql\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m valid_dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgretelai/synthetic_text_to_sql\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m test_dataset  = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mgretelai/synthetic_text_to_sql\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/load.py:1424\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1420\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1421\u001b[39m keep_in_memory = (\n\u001b[32m   1422\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1423\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m ds = \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/builder.py:1087\u001b[39m, in \u001b[36mDatasetBuilder.as_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, in_memory)\u001b[39m\n\u001b[32m   1084\u001b[39m verification_mode = VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS)\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m datasets = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1099\u001b[39m     datasets = DatasetDict(datasets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/utils/py_utils.py:493\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    492\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    495\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/builder.py:1117\u001b[39m, in \u001b[36mDatasetBuilder._build_single_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, in_memory)\u001b[39m\n\u001b[32m   1114\u001b[39m     split = Split(split)\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post_processing_resources(split).values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/builder.py:1191\u001b[39m, in \u001b[36mDatasetBuilder._as_dataset\u001b[39m\u001b[34m(self, split, in_memory)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_legacy_cache():\n\u001b[32m   1190\u001b[39m     dataset_name = \u001b[38;5;28mself\u001b[39m.name\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m dataset_kwargs = \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1197\u001b[39m fingerprint = \u001b[38;5;28mself\u001b[39m._get_dataset_fingerprint(split)\n\u001b[32m   1198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint=fingerprint, **dataset_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/arrow_reader.py:248\u001b[39m, in \u001b[36mBaseReader.read\u001b[39m\u001b[34m(self, name, instructions, split_infos, in_memory)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    229\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m     in_memory=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    233\u001b[39m ):\n\u001b[32m    234\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m \u001b[33;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[32m    250\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInstruction \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m corresponds to no data!\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/arrow_reader.py:221\u001b[39m, in \u001b[36mBaseReader.get_file_instructions\u001b[39m\u001b[34m(self, name, instruction, split_infos)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[32m    220\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     file_instructions = \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     files = file_instructions.file_instructions\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/arrow_reader.py:130\u001b[39m, in \u001b[36mmake_file_instructions\u001b[39m\u001b[34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[39m\n\u001b[32m    128\u001b[39m     instruction = ReadInstruction.from_spec(instruction)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m absolute_instructions = \u001b[43minstruction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[32m    133\u001b[39m file_instructions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/arrow_reader.py:620\u001b[39m, in \u001b[36mReadInstruction.to_absolute\u001b[39m\u001b[34m(self, name2len)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[32m    609\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[32m    610\u001b[39m \n\u001b[32m    611\u001b[39m \u001b[33;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m \u001b[33;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._relative_instructions]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/datasets/arrow_reader.py:437\u001b[39m, in \u001b[36m_rel_to_abs_instr\u001b[39m\u001b[34m(rel_instr, name2len)\u001b[39m\n\u001b[32m    435\u001b[39m split = rel_instr.splitname\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown split \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    438\u001b[39m num_examples = name2len[split]\n\u001b[32m    439\u001b[39m from_ = rel_instr.from_\n",
      "\u001b[31mValueError\u001b[39m: Unknown split \"validation\". Should be one of ['train', 'test']."
     ]
    }
   ],
   "source": [
    "# jsonl ÌååÏùºÏùÑ Î∂àÎü¨ÏôÄ dataset ÏÉùÏÑ±\n",
    "dataset = Dataset.from_json(\"./ecommerce_data/ecommerce_finetune.jsonl\")\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split=\"train\")\n",
    "test_dataset  = load_dataset(\"gretelai/synthetic_text_to_sql\", split=\"test\")\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e388de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train(train_data):\n",
    "    return f\"ÏßàÎ¨∏: {train_data['input']}\\nÎãµÎ≥Ä: {train_data['output']}\\n<END>\"\n",
    "\n",
    "def tokenize_func(train_data):\n",
    "    return tokenizer(\n",
    "        format_train(train_data),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,   # Î™®Îç∏ context ÌÅ¨Í∏∞Ïóê ÎßûÍ≤å Ï°∞Ï†ï\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "eos_token_id=tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=15,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1919fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangjoon/.pyenv/versions/ai_practice/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/750 01:54 < 01:27, 3.69 it/s, Epoch 28.33/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy}\n\u001b[32m     13\u001b[39m trainer = Trainer(\n\u001b[32m     14\u001b[39m     model=model,\n\u001b[32m     15\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtraining done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/transformers/trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ai_practice/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    mask = labels != -100\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80ba44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:262145 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Output\n",
      "ÏßàÎ¨∏: ÏûëÎÖÑ Ïã†Í∑ú Í∞ÄÏûÖ Í≥†Í∞ù ÏàòÎäî?\n",
      "ÎãµÎ≥Ä: 2019ÎÖÑ 12Ïõî 10Ïùº\n",
      "\n",
      "ÏßàÎ¨∏: 2019ÎÖÑ 12Ïõî 10Ïùº\n",
      "ÎãµÎ≥Ä: 2019ÎÖÑ 12Ïõî 10Ïùº\n",
      "\n",
      "ÏßàÎ¨∏: 2019ÎÖÑ 12Ïõî\n",
      "Generated SQL\n",
      "2019ÎÖÑ 12Ïõî 10Ïùº\n",
      "\n",
      "ÏßàÎ¨∏: 2019ÎÖÑ 12Ïõî 10Ïùº\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ÏûëÎÖÑ Ïã†Í∑ú Í∞ÄÏûÖ Í≥†Í∞ù ÏàòÎäî?\"\n",
    "format_input = f\"ÏßàÎ¨∏: {input_text}\\nÎãµÎ≥Ä:\"\n",
    "inputs = tokenizer(format_input, return_tensors=\"pt\")\n",
    "device = model.device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,       # ÏÉòÌîåÎßÅ ÎπÑÌôúÏÑ±Ìôî\n",
    "    num_beams=1,           # Greedy Search\n",
    "    temperature=None,      # ÏïÑÏòà Ï†úÍ±∞\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Full Output\")\n",
    "print(result)\n",
    "\n",
    "# \"ÎãµÎ≥Ä:\" Îí§Îßå Ï∑®Ìï®\n",
    "if \"ÎãµÎ≥Ä:\" in result:\n",
    "    result = result.split(\"ÎãµÎ≥Ä:\")[1].strip()\n",
    "\n",
    "# SELECT ~ ; Ìå®ÌÑ¥Îßå Ï∂îÏ∂ú\n",
    "match = re.search(r\"(SELECT[\\s\\S]*?;)\", result, re.IGNORECASE)\n",
    "if match:\n",
    "    sql_query = match.group(1).strip()\n",
    "else:\n",
    "    sql_query = result.strip()\n",
    "\n",
    "print(\"Generated SQL\")\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cab90db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:262145 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi! how are you?\n",
      "i hope you are doing well.\n",
      "i hope you are having a good day.\n",
      "i hope you are having a good day.\n",
      "i hope you are having a good day.\n",
      "i hope you are having a good day.\n",
      "i hope you are having a good day.\n",
      "i hope you are having a\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hi! how are you?\"\n",
    "format_input = f\"{input_text}\\n\"\n",
    "inputs = tokenizer(format_input, return_tensors=\"pt\")\n",
    "device = model.device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,       # ÏÉòÌîåÎßÅ ÎπÑÌôúÏÑ±Ìôî\n",
    "    num_beams=3,           # Greedy Search\n",
    "    temperature=None,      # ÏïÑÏòà Ï†úÍ±∞\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fd6e6",
   "metadata": {},
   "source": [
    "#### RAG\n",
    "\n",
    "1. ÌååÏùº Î°úÎìú\n",
    "\n",
    "2. ÏûÑÎ≤†Îî© :: vector DB Î•º ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò / Ï†ÄÏû•\n",
    "\n",
    "3. vector DB Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ :: chroma ÏÇ¨Ïö© -> in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b034da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ÌååÏùº Î°úÎìú\n",
    "rag_dataset = load_dataset(\"json\", data_files=\"./ecommerce_data/ecommerce_schema_rag.jsonl\")\n",
    "\n",
    "print(rag_dataset)\n",
    "\n",
    "\n",
    "# ÏûÑÎ≤†Îî© :: vector DB Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌï¥ Îç∞Ïù¥ÌÑ∞Î•º Î≥ÄÌôò / Ï†ÄÏû•\n",
    "# vector DB Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ :: chroma ÏÇ¨Ïö© -> in memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
