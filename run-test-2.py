import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig


LOCAL_MODEL_PATH = "../ai_models/gemma-3-270m"
DTYPE = torch.float32
MODEL_OPTION = {"use_safetensors": True}
ADAPTER_FLAG = False
ADAPTER_PATH = ""

device = None
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("âœ… Using Apple Silicon GPU (MPS)")
else:
    device = torch.device("cpu")
    print("âš ï¸ MPS not available; using CPU")

# ==== í† í¬ë‚˜ì´ì € ë¡œë“œ ====
def get_tokenizer(model_path):
    print("ğŸ”„ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=True,
        padding_side="left",  # ë°°ì¹˜ ì¶”ë¡  ëŒ€ë¹„ ì•ˆì „
        use_safetensors=True,
    )
    if tokenizer.pad_token is None:
        print("âš ï¸ pad_tokenì´ ì—†ì–´ì„œ eos_tokenìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    tokenizer.padding_side = "left"
    return tokenizer

def get_model(model_path, dtype, option):
    print("ğŸ”„ Loading model...")
    return AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=dtype,
        low_cpu_mem_usage=True,
        use_safetensors=option["use_safetensors"],
    )

def set_model_to_device(model, device):
    print("ğŸ”„ Moving model to device...")
    model.to(device)
    model.eval()
    return model

tokenizer = get_tokenizer(LOCAL_MODEL_PATH)
model = get_model(LOCAL_MODEL_PATH, DTYPE, MODEL_OPTION)
model = set_model_to_device(model, device)

print("load done.")

# ==== í”„ë¡¬í”„íŠ¸ êµ¬ì„±: chat í…œí”Œë¦¿ ìë™ ê°ì§€ ====
def build_inputs(user_text: str):
    # tokenizerê°€ chat í…œí”Œë¦¿ì„ ì œê³µí•˜ë©´ ê·¸ê±¸ ì‚¬ìš© (instruct ëª¨ë¸ì— ìœ ë¦¬)
    has_chat = hasattr(tokenizer, "chat_template") and tokenizer.chat_template
    if has_chat:
        messages = [{"role": "user", "content": user_text}]
        enc = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        )
    else:
        # ë² ì´ìŠ¤ ëª¨ë¸ì¼ ê²½ìš° ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸
        enc = tokenizer(
            user_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        )
    return enc

# ==== ìƒì„± ì„¤ì • ====
gen_cfg = GenerationConfig(
    max_new_tokens=256,       # ë„ˆë¬´ ê¸¸ë©´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ 128~256 ê¶Œì¥
    temperature=0.7,
    top_p=0.9,
    do_sample=True,           # ë‹¤ì–‘ì„± í™•ë³´
    repetition_penalty=1.05,  # ì¤‘ë³µ ì–µì œ ì‚´ì§
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,  # pad ê²½ê³  ë°©ì§€
)

user_input = "ì•ˆë…•í•˜ì„¸ìš”. ë‹¹ì‹ ì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?"
inputs = build_inputs(user_input).to(device)

# ==== ìƒì„± ====
print("ğŸš€ Generating...")
with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=gen_cfg.max_new_tokens,
        temperature=gen_cfg.temperature,
        top_p=gen_cfg.top_p,
        do_sample=gen_cfg.do_sample,
        repetition_penalty=gen_cfg.repetition_penalty,
        eos_token_id=gen_cfg.eos_token_id,
        pad_token_id=gen_cfg.pad_token_id,
    )

# ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œì™¸)
gen_only = output_ids[0][inputs["input_ids"].shape[-1]:]
text = tokenizer.decode(gen_only, skip_special_tokens=True)

print("\n===== MODEL OUTPUT =====")
print(text)
print("========================")